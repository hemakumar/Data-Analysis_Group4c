%\doublespacing

 %\section{Data Visualization}
 %An analysis suite, written in Haskell and {\tt Gnuplot}, has been created to
 %produces quantitative visualizations of various cooling related metrics concerning the given {\tt RAWDATA}, described below. It uses temporal data-mining technique to produce a novel visualization of cluster overlays on the time-series data for five chiller metrics, in an attempt to discover `motifs' (inspired by work found in \cite{Patnaik2009}).

\section*{Data}
We are using two postmortem datasets collected at the PNNL's ESDC:
\verb=LD_A1_56p_2ppn_28n_IO-BASIC_even_RAWDATA= (LD) and \verb=HD_A1_224p_8ppn_28n_RAWDATA= (HD).
This data seems to correspond to the experiments in \cite{Karavanic2011}: 

  %\item Postmortem data gotten from \verb=/u/karavan/Public= at \verb=rita.cat.pdx 
  \begin{itemize}

 \item The LD dataset---Low-Density Even-Distribution Workload---was collected by FRED \date{2011-05-05}.
  \item The HD dataset---High-Density Workload---was collected by FRED \date{2011-04-25}.
  \item  \begin{quote}
      All of the power and cooling data measured by FRED [Fundamental Research in Energy efficient Data centers, an analysis software tool developed at PNNL] sensors is collected into a central FRED database, using an independent data network.    
    \end{quote}

  \item The workloads consist of MPI tasks split into three trials t01--03.
  \item   
    \begin{quote}
      In the HD schedules, the 224 processes ran on 28 nodes (eight processes per node) all housed in a single rack.
    \end{quote}
\item
   \begin{quote}
     The nodes are connected via a DDR InfiniBand 1-layer fat tree; one rack (A2) contains the networking equipment and seven racks (A1, A3--4, B1--4) are for the compute nodes.
   \end{quote}

 \item
\begin{quote}
In the low density LD schedules, the 224 processes executed on 112 nodes, with two processes per node, and the processes were spread across racks B1, B2, B3, and B4.
 \end{quote}  
 \item
 \begin{quote}
Low Density Even Distribution -- In this set, we used
racks B1, B2, B3, and B4; and we scheduled two MPI
tasks per node, using all of the nodes in all four racks.
 \end{quote}

     \end{itemize}


 \subsection*{Power \& Cooling}

 Power consumption is a likely concern for the ESDC; though only 7.63\textcent{} per kWh, in Oregon. Tables \ref{t1} and \ref{t2} include the average power consumption of each rack over the LD and HD trials according to the postmortem data. % It was hard to tell whether the gradient of one line was steeper than another, in the LD set; hence the accompanying tables were created to show overall kWh increase per trial, and to further scrutinize power consumption within the racks. 


A sure concern for the ESDC is overheating.
Air cooling attempts to dissipate some of the heat generated by the CPUs. A useful metric for estimating cooling effort is the Node Air Delta.
Tables \ref{t1} and \ref{t2} include the average CPU temperatures and Node Air Delta of each rack over the LD and HD trials. Unfortunately, the postmortem data is badly corrupted by faulty sensor readings.
Negative, and ludicrously high, temperature values had to be filtered from the true readings. For CPU temperature, the filtering range was $72<temp<150$.
For Node Air Delta, the filtering range was $0<\Delta temp<40$.

\begin{table}[!h]
  \centering
   \centerline{Total kWh increase.}
   \input{table/ld/powerunit_powerkwhDeltaAvg.tex}\\
   \vspace{1cm}
   \centerline{Average Per-Node, Per-CPU, CPU Temperature Average for Each Rack (within $72<{\tt Temp}<150$)}
   \input{table/ld/cpu_tempAvgAvg.tex}\\
   \vspace{1cm}
   \centerline{Maximum Per-Node, Per-CPU, CPU Temperature Maximum for Each Rack (within $72<{\tt Temp}<150$)}
   \input{table/ld/cpu_tempMaxMax.tex}\\
   \vspace{1cm}
   \centerline{Average Per-Node Air Temperature Delta for Each Rack (within $0<\Delta{\tt Temp}<40$)}
   \input{table/ld/nodeair_deltaAvgAvg.tex}\\
   \caption{{\bf Low-Density Workload.} To filter out faulty sensor readings, the component temperatures counted do not include every node in every rack; see the counts below each table.\label{t1}}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!h]
  \centering
   \centerline{Total kWh increase.}
   \input{table/hd/powerunit_powerkwhDeltaAvg.tex}\\
   \vspace{1cm}
   \centerline{Average Per-Node, Per-CPU, CPU Temperature Average for Each Rack (within $72<{\tt Temp}<150$)}
   \input{table/hd/cpu_tempAvgAvg.tex}\\
   \vspace{1cm}
   \centerline{Maximum Per-Node, Per-CPU, CPU Temperature Maximum for Each Rack (within $72<{\tt Temp}<150$)}
   \input{table/hd/cpu_tempMaxMax.tex}\\
   \vspace{1cm}
   \centerline{Average Per-Node Air Temperature Delta for Each Rack (within $0<\Delta{\tt Temp}<40$)}
   \input{table/hd/nodeair_deltaAvgAvg.tex}\\
   \caption{{\bf High-Density Workload.} To filter out faulty sensor readings, the component temperatures counted do not include every node in every rack; see the counts below each table.\label{t2}}


\end{table}



The racks equipped with TMUs provide additional cooling infrastructure.
The degree of cooling which the TMU provides is captured by the water-temperature-delta metric. Another useful metric is the manifold pressure---the mass flow rate of hot/cold water.
Only the LD dataset had useful TMU readings.

%Evidently, trial 3 (t03) involved slightly more energy intensive application processes.

%Are we ever going to get PerfTrack and have the data organized in PostgreSQL?
%perhaps it will be easy to gather the data necessary to make a chart like the one depicted on page 440 of \url{http://web.cecs.pdx.edu/~karavan/csg533/karavanic_PDCS2011.pdf}.
% Chiller utilization measurements were collected by FRED at 30 second intervals; so, for simplicity, each trial's time series starts at 0 and is incremented accordingly.
Overall room performance, concerning power and cooling, is best captured by chiller metrics.
 \subsection*{Chillers}

Within the cooling infrastructure, most of the energy is spent on chillers, which refrigerate the water used to extract heat from the equipment in the data center.
 Chillers keep data centers from overheating and hence keep them properly functioning.
 %Chiller metrics capture the overall performance of the data center's cooling infrastructure.
 While one unit may be sufficient for a small data center, several units operating as an ensemble are required to satisfy the cooling demand of a large data center.

  The ESDC is equipped with 5 chillers units. 
 Three chiller units ($C_1,C_2,C_4$) were used for the LD trials, and only two ($C_1,C_4$) were used for the HD trials---as evidenced by KW consumption. 
The following chiller metrics, derived from \cite{Patnaik2011}, were extracted from the LD and HD datasets.
 \begin{itemize}
   \item
   {\bf Data-center cooling load.} This is the amount of heat that is generated (and thus needs to be dissipated) at a data center. It is approximately equivalent to the power consumed by the equipment since almost all of it is dissipated as heat. It is commonly specified in kilowatts (KW).
 \item
   {\bf COP.} The coefficient of performance (COP) of a chiller unit indicates how efficiently the unit provides cooling, and, is defined as the ratio between the cooling provided and the power consumed, i.e.,
   \begin{equation*}
     COP_i = \frac{L_i}{P_i}\label{cop}
 \end{equation*}
where $L_i$ is the cooling load on the $i$th chiller unit and $P_i$ is the power consumed by it.
\item {\bf Chiller utilization.} Chiller utilization depends on the degree of cooling provided; i.e., the difference between the inlet and outlet water temperatures ({\tt CHILLER WATER-TEMP DELTA} and {\tt CONDENSER WATER-TEMP DELTA}).
\end{itemize}

When these chiller metrics are plotted, oscillations can be observed.
     High amplitude and frequent variations in utilization due to varying load or some failure condition result in decreased lifespan, and, thus, need to be minimized.
 Although operating curves for individual chiller units exist, no model is available for operation of an ensemble, especially one consisting of heterogeneous units.



 \section*{Data Mining}
\newtheorem{mydef}{Definition}


\begin{mydef} 
   A multivariate time series $T = \langle t_1,\dots, t_m\rangle$ is an ordered set of real-valued vectors of a particular variable. Each real-valued vector $t_i$ captures the utilizations across all the chiller units.
 \end{mydef}

     \begin{quote}
Motifs are repetitive patterns of occurrence in time-series data. In understanding multivariate time-series data about chiller utilizations, we seek to identify motifs that underly how different chillers are involved in meeting the varying demand posed by data centers. We would like to identify regions of time-series progression that demonstrate better/improved sustainability measures than others
   \cite[p. \;34:7]{Patnaik2011}.\end{quote}


 \subsection*{Approach}
% Gaussian clusters discovered by EM clustering with $k=3$, over the three trials for the Chiller metrics, produced the best results.  
First, chiller metrics are extracted from the datasets by scripting into their file systems and manipulating the data as vectors with Haskell.
For clustering, ELKI (release 0.4.1 \cite{Achtert}) is used. The algorithms being used are: $k$-means and expectation maximum (EM). 
A Haskell script formats metric data and feeds it to ELKI.
The input data is labeled in such a way that the time-series information can be retrieved after clustering; hence making it possible for cluster transitions to be overlaid on the time series across each of the three trials.
ELKI produces clustering files. Then a Haskell script reads all the cluster files and stores the data points in a Map according to their cluster labels {\tt (trial, SEC)}. Next, {\tt Gnuplot} dat files are created for each trial by looking up data points in the Map and sorting them by time.
The cluster transitions can be seen by overlaying the time series with shades of color representing clusters. 
{\tt Gnuplot} scripts are generated to do this shading. Finally, the Haskell script executes {\tt Gnuplot} on all the {\tt Gnuplot} scripts, which are rendered as PNG images.

This gives an intuitive feel for how a learning algorithm could be built to detect motifs:
You look at the time series to detect motifs, then see if they are captured by transitions of cluster shadings.
These transitions can be used to encode the multivariate numeric data into symbolic form, which can serve as the input to frequent episode mining.
The color shadings make it easy for a human to see what possible motifs the AI would choose using various clustering parameters, hence the human can guide the AI to use the best clustering parameters. 
Colors had to be chosen carefully to be distinguishable and not clash with the plots. The color schemes used were derived from \url{http://www.sron.nl/~pault/colourschemes.pdf}.


\begin{itemize}
  \item Code $\to$ \url{https://github.com/mmlxiv/Data-Analysis_Group4c}
\end{itemize}
%Note: TMU metrics could be analysed in the same way as chiller metrics; but, this was left undone because 
%the HD dataset did not contain usefull TMU metrics.
%It is kind of futile trying to find motifs in the LD dataset, at such a fine granularity with such short lived trials.
% Naively thinking the three trials for each workload (LD \& HD) could be clustered on their chiller metrics led to the automated analysis captured in the cluster-overlayed time-series figures in Appendix D \& F.




 \section*{Reflections \& Future}
This project was a first step toward developing algorithms for processing multivariate time-series data to characterize sustainability measures of the patterns mined.
\begin{itemize}
  \item TMU metrics could be analysed in the same way as chiller metrics; this was left undone because 
the HD dataset did not contain useful TMU metrics.
%\item It is kind of futile trying to find motifs in the LD dataset, at such a fine granularity with such short lived trials.
   \item It would be nice if FRED measurements were stored in a NoSQL graph database. Having the data structured as a graph seems like it would make the data easier to navigate. 
   \item Mining FRED sensor streams, directly, could give a real-time perspective into ESDC behavior so that strategies could be identified to improve efficiency metrics.
\item  Meaningful analysis might be carried out with a more extensive amount of ESDC data.
Example: \cite[pp. \;34:14--34:16 (see Tables I--V)]{Patnaik2011}

  %  \item The temporal data-mining technique of overlaying clusters on the time series, described in \cite{Patnaik2009,Patnaik2011}, was tested on Chiller data. Though, it was difficult to find motifs at such a fine granularity, with our limited data.
  
\item An advanced machine learning infrastructure could be built using Dynamic Bayesian Networks to infer diagnostic predictions (e.g. the CAMAS framework in \cite[p. \;34:8 (see Fig 5.)]{Patnaik2011}).
\end{itemize}


     % \item It would be nice if we could get at the FRED measurements through a cloud-based data store---perhaps a graph database.  
  
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



